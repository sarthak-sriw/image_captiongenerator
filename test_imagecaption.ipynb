{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_imagecaption.ipynb",
      "provenance": [],
      "mount_file_id": "11Wfgh_ClZVCViuv3gn-dc9fsf3HkfFuI",
      "authorship_tag": "ABX9TyM4mEHwMvnrSvYoSeBiDGP1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthak-sriw/image_captiongenerator/blob/main/test_imagecaption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVFOL6thZBkT",
        "outputId": "2c42814b-94bc-4ccb-e6e6-f2e4be873915"
      },
      "source": [
        "from pickle import load\r\n",
        "from numpy import argmax\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.applications.vgg16 import VGG16\r\n",
        "from keras.preprocessing.image import load_img\r\n",
        "from keras.preprocessing.image import img_to_array\r\n",
        "from keras.applications.vgg16 import preprocess_input\r\n",
        "from keras.models import Model\r\n",
        "from keras.models import load_model\r\n",
        "\r\n",
        "# extract features from each photo in the directory\r\n",
        "def extract_features(filename):\r\n",
        "\t# load the model\r\n",
        "\tmodel = VGG16()\r\n",
        "\t# re-structure the model\r\n",
        "\tmodel.layers.pop()\r\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\r\n",
        "\t# load the photo\r\n",
        "\timage = load_img(filename, target_size=(224, 224))\r\n",
        "\t# convert the image pixels to a numpy array\r\n",
        "\timage = img_to_array(image)\r\n",
        "\t# reshape data for the model\r\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\r\n",
        "\t# prepare the image for the VGG model\r\n",
        "\timage = preprocess_input(image)\r\n",
        "\t# get features\r\n",
        "\tfeature = model.predict(image, verbose=0)\r\n",
        "\treturn feature\r\n",
        "\r\n",
        "# map an integer to a word\r\n",
        "def word_for_id(integer, tokenizer):\r\n",
        "\tfor word, index in tokenizer.word_index.items():\r\n",
        "\t\tif index == integer:\r\n",
        "\t\t\treturn word\r\n",
        "\treturn None\r\n",
        "\r\n",
        "# generate a description for an image\r\n",
        "def generate_desc(model, tokenizer, photo, max_length):\r\n",
        "\t# seed the generation process\r\n",
        "\tin_text = 'startseq'\r\n",
        "\t# iterate over the whole length of the sequence\r\n",
        "\tfor i in range(max_length):\r\n",
        "\t\t# integer encode input sequence\r\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\r\n",
        "\t\t# pad input\r\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\r\n",
        "\t\t# predict next word\r\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\r\n",
        "\t\t# convert probability to integer\r\n",
        "\t\tyhat = argmax(yhat)\r\n",
        "\t\t# map integer to word\r\n",
        "\t\tword = word_for_id(yhat, tokenizer)\r\n",
        "\t\t# stop if we cannot map the word\r\n",
        "\t\tif word is None:\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# append as input for generating the next word\r\n",
        "\t\tin_text += ' ' + word\r\n",
        "\t\t# stop if we predict the end of the sequence\r\n",
        "\t\tif word == 'endseq':\r\n",
        "\t\t\tbreak\r\n",
        "\treturn in_text\r\n",
        "\r\n",
        "# load the tokenizer\r\n",
        "tokenizer = load(open('/content/drive/MyDrive/ImageCaptionproj/tokenizer.pkl', 'rb'))\r\n",
        "# pre-define the max sequence length (from training)\r\n",
        "max_length = 34\r\n",
        "#load the model\r\n",
        "model = load_model('/content/drive/MyDrive/ImageCaptionproj/model_19.h5')\r\n",
        "# load and prepare the photograph\r\n",
        "photo = extract_features('/content/drive/MyDrive/ImageCaptionproj/download (3).jpg')\r\n",
        "# generate description\r\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\r\n",
        "print(description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb20b067e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "startseq man in black shirt is riding bike on the street endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfWN6fynZfq8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}